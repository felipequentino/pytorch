{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1) Design model (input, output size, forward pass)\n# 2) Construc loss and optimizer\n# 3) Training loop\n#  - forward pass: compute prediction and loss\n#  - backward pass: gradients\n#  - update weights\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-26T17:37:08.893672Z","iopub.execute_input":"2024-08-26T17:37:08.894049Z","iopub.status.idle":"2024-08-26T17:37:09.001494Z","shell.execute_reply.started":"2024-08-26T17:37:08.894014Z","shell.execute_reply":"2024-08-26T17:37:09.000406Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# 0) prepare data\nbc = datasets.load_breast_cancer()\nX, y = bc.data, bc.target\n\nn_samples, n_features = X.shape\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:38:17.656408Z","iopub.execute_input":"2024-08-26T17:38:17.657737Z","iopub.status.idle":"2024-08-26T17:38:17.674655Z","shell.execute_reply.started":"2024-08-26T17:38:17.657685Z","shell.execute_reply":"2024-08-26T17:38:17.673240Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# scale\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\n\ny_train = y_train.view(y_train.shape[0], 1)\ny_test = y_test.view(y_test.shape[0], 1)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:40:04.396655Z","iopub.execute_input":"2024-08-26T17:40:04.397118Z","iopub.status.idle":"2024-08-26T17:40:04.409302Z","shell.execute_reply.started":"2024-08-26T17:40:04.397075Z","shell.execute_reply":"2024-08-26T17:40:04.408058Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# 1) model\n# f = wx + b, sigmoid at the end\n\nclass LogisticRegression(nn.Module):\n    \n    def __init__(self, n_input_features):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(n_input_features, 1)\n    \n    def forward(self, x):\n        y_predicted = torch.sigmoid(self.linear(x))\n        return y_predicted\n\nmodel = LogisticRegression(n_features)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:44:24.555254Z","iopub.execute_input":"2024-08-26T17:44:24.555696Z","iopub.status.idle":"2024-08-26T17:44:24.574924Z","shell.execute_reply.started":"2024-08-26T17:44:24.555655Z","shell.execute_reply":"2024-08-26T17:44:24.573516Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# 2) loss and optimizer\nlearning_rate = 0.01\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:44:45.524190Z","iopub.execute_input":"2024-08-26T17:44:45.524841Z","iopub.status.idle":"2024-08-26T17:44:45.531956Z","shell.execute_reply.started":"2024-08-26T17:44:45.524795Z","shell.execute_reply":"2024-08-26T17:44:45.530392Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### loss.backward(): Calcula os gradientes dos parâmetros do modelo com base na função de perda.\n### optimizer.step(): Aplica esses gradientes aos parâmetros do modelo, ajustando-os para minimizar a perda","metadata":{}},{"cell_type":"code","source":"# 3) training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # forward pass and loss\n    y_predicted = model(X_train)\n    loss = criterion(y_predicted, y_train)\n    \n    # backward pass\n    loss.backward()\n    \n    # updates\n    optimizer.step()\n    \n    # zero gradients\n    optimizer.zero_grad()\n    \n    if (epoch + 1)% 10 == 0:\n        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n\nwith torch.no_grad():\n    y_predicted = model(X_test)\n    y_predicted_cls = y_predicted.round()\n    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n    print(f'accuracy = {acc:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:48:37.780114Z","iopub.execute_input":"2024-08-26T17:48:37.780527Z","iopub.status.idle":"2024-08-26T17:48:37.911703Z","shell.execute_reply.started":"2024-08-26T17:48:37.780493Z","shell.execute_reply":"2024-08-26T17:48:37.910404Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"epoch: 10, loss = 0.5345\nepoch: 20, loss = 0.4480\nepoch: 30, loss = 0.3912\nepoch: 40, loss = 0.3509\nepoch: 50, loss = 0.3206\nepoch: 60, loss = 0.2969\nepoch: 70, loss = 0.2777\nepoch: 80, loss = 0.2618\nepoch: 90, loss = 0.2484\nepoch: 100, loss = 0.2368\naccuracy = 0.8947\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}